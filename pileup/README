Instructions to produce PU histos for DT (7.10.2016, Hannu Siikonen)
Updated 25.5.2018

The file locations have changed. Instructions to be updated well!

Old insttructions

Note:

The user may choose to use puhist.sh or puhist_all.sh
The latter is slower, as it checks each trigger version separately.
For the latter to work, one needs to supply the "per-version" luminosity weights in settings.h
For the former to work, one needs to supply the "per-year" luminosity weights in settings.h
The weights are necessary to obtain approximately the same integrals for the various triggers.
The integrals are not completely equal, which might be due to smearing effects.
If one cannot provide the weights, one could scale all resulting histograms with their integrals.
However, if done so, the user needs to simplify the provided code and understand what (s)he is doing.

Step0:

Create a working directory under the pileup directory and copy all the scripts there.
Enter this directory.

Step1:

Check which certificate you want to use.
This path is given in puhist*.sh
Change also the normtag and pileup files if needed.

Step2:

Go to CMS DAS and search for and look for: (here 2016B and PromptReco-v1 are just an example)

dataset=/JetHT/Run2016B-*/MINIAOD and select some of these datasets to work with (preferably PromptReco)

continue by asking (in plain mode)
run,lumi dataset=/JetHT/Run2016B-PromptReco-v1/MINIAOD

Finally, create a folder RunB/ (or such) and put the plain run,lumi output into a file lumis.txt.
This is not a JSON but something close to that, which is easy to process.

Repeat this to all the datasets you want to use. Put the folder names in use into the FOLDERS
array given in the beginning of puhistos.sh.

Far in the future also the trigger names might change, so also these need to be checked.

Step3:

Make sure that you have a CMSSW environtment installed and run 'cmsenv'

Run 'bash puhist.sh' and hope for the best.

- In general tons of error messages may be produced. Sometimes running python on lxplus generates
some error messages that needn't be worried about. There are also error messages about missing
events and over-one-weights. Some of these are related to the HLT selection etc. However, one
should be cautious with all these errors flying around.

Step4:

Go to each folder (e.g. RunB) separately and run:
root -l -b -q ../puhist_combine.C or
root -l -b -q ../puhist_combineversions.C
(pileup_DTu.root is created)

Then run:
root -l -b -q ../puhist_normalize.C
(pileup_DT.root is created)

Note:

If one wishes to use a pileup profile for many runs, one should create a folder (e.g RunBCD),
enter the folder and run:
hadd pileup_DTu.root ../RunB/pileup_DTu.root ../RunC/pileup_DTu.root ../RunD/pileup_DTu.root
and finally
root -l -b -q ../puhist_normalize.C

It is imperative that no normalization is done before one is at the level for which
the MC tuple is processed. If one performs premature normalization, the weights
of different runs will be biased w.r.t data.

If the weighting becomes too involved, one can survive without it. The weighting is just
meant to give convenient relative weights in for the data processing.

Step5:

Make sure that you have also generated MC pu histos. Basically this is done by

ProcessedTree->Draw("EvtHdr_.mTrPu>>pileupmc(80,0,80)")
TH1D* th = gROOT->FindObject("pileupmc")

from within the SMP jet tuples.

For the pthat sliced tuples one can utilize the code pufromslices.C
The pufromslices shows the pileup profiles also for each slice separately,
but the slices are mostly very similar to each other.

If dealing with a new MC campaign, remember to run GetSliceEvts.C and update
the slice information in settings.h before running pufromslices.C.
